{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector,Embedding,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextProcesser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.word_index = dict() ;\n",
    "        self.word_index[\"<PAD>\"] = 0 #Padding\n",
    "        self.word_index[\"<START>\"] = 1 #start\n",
    "        self.word_index[\"<UNK>\"] = 2  # unknown\n",
    "        self.word_index[\"<UNUSED>\"] = 3\n",
    "        self.word_index[\"<EOS>\"] = 4\n",
    "        self.curr_index = 4 ;\n",
    "        \n",
    "        self.rev_word_index = dict()\n",
    "        self.rev_word_index[0] = \"<PAD>\"\n",
    "        self.rev_word_index[1] = \"<START>\"\n",
    "        self.rev_word_index[2] = \"<UNK>\"\n",
    "        self.rev_word_index[3] = \"<UNUSED>\"\n",
    "        self.rev_word_index[4] = \"<EOS>\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_word_list(self,sentence):\n",
    "        \n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        #Remove Punctuation\n",
    "        sentence = sentence.translate(str.maketrans('','','!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'))\n",
    "        \n",
    "        #Remove Whitespaces\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        #Tokenize the sentences \n",
    "        tokens = nltk.tokenize.word_tokenize(sentence) # \n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "    def fit_on_text(self,sentence):\n",
    "        \n",
    "        word_list = self.get_word_list(sentence)\n",
    "        \n",
    "        for word in word_list:\n",
    "            \n",
    "            if word not in self.word_index:\n",
    "                self.curr_index = self.curr_index+1 \n",
    "                \n",
    "                self.word_index[word] = self.curr_index \n",
    "                self.rev_word_index[self.curr_index] = word\n",
    "                \n",
    "                \n",
    "        return word_list\n",
    "        \n",
    "    def get_seq_from_text(self,sentence,max_len=256):\n",
    "        \n",
    "        word_list = self.fit_on_text(sentence)\n",
    "        \n",
    "        seq = [self.word_index[\"<START>\"] ]\n",
    "        for word in word_list:\n",
    "            if word is '.':\n",
    "                seq.append(self.word_index[\"<EOS>\"])\n",
    "            else:\n",
    "                seq.append(self.word_index[word])\n",
    "        \n",
    "        return seq\n",
    "    \n",
    "    def proccess_text(self,file_names=None,text=None,max_len=256):\n",
    "        \n",
    "        seqs = []\n",
    "        \n",
    "        if file_names :\n",
    "            \n",
    "            for file_name in file_names:\n",
    "                with open(filename,'r') as file:\n",
    "                    text = file.read()\n",
    "                    seq = self.get_seq_from_text(text)\n",
    "                seqs.append(seq)\n",
    "        elif text:\n",
    "            \n",
    "            seq = self.get_seq_from_text(text)\n",
    "            seqs.append(seq)\n",
    "        else:\n",
    "            print(\"file_names = None and text = None\")\n",
    "            \n",
    "        seqs = sequence.pad_sequences(seqs,value=self.word_index[\"<PAD>\"],padding='post',maxlen=max_len)\n",
    "        return seqs\n",
    "    \n",
    "    def decode_sequence(self,seq):\n",
    "        \"\"\"\n",
    "            seq : numpy array of shape = (seq_length,)\n",
    "        \"\"\"\n",
    "        line = []\n",
    "        for i in range(seq.shape[0]):\n",
    "            index = seq[i]\n",
    "            if index >= 0 and index <=4:\n",
    "                \n",
    "                #This block can be used to handle the special values <START> , <PAD> , <EOS> \n",
    "                if index == 4:\n",
    "                    line.append('.')\n",
    "            else:\n",
    "                line.append(self.rev_word_index[index])\n",
    "                \n",
    "        \n",
    "        return ' '.join(line)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_length,vocab_size,latent_dim):\n",
    "\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    x = Embedding(vocab_size,latent_dim,input_length=input_length,name=\"encoder_embedding\")(inputs)\n",
    "\n",
    "    encoded = Bidirectional(LSTM(latent_dim), merge_mode=\"sum\",name=\"encoder\")(x)\n",
    "\n",
    "\n",
    "    decoder_input = RepeatVector(input_length, name=\"repeater\")(encoded)\n",
    "    decoded = Bidirectional(LSTM(latent_dim), merge_mode=\"sum\",name=\"decoder\")(decoder_input)\n",
    "\n",
    "    autoencoder = Model(inputs,decoded)\n",
    "    \n",
    "    return autoencoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextProcesser()\n",
    "def load_dataset():\n",
    "    \n",
    "    train_data = []\n",
    "    with open('Data/desc.txt','r') as file:\n",
    "        for line in file:\n",
    "            data = tokenizer.proccess_text(text=line)\n",
    "            train_data.extend([data])\n",
    "        \n",
    "    return np.array(train_data)\n",
    "\n",
    "data = load_dataset()\n",
    "data = data.reshape(data.shape[0],data.shape[2])\n",
    "\n",
    "with open('tokenizer.pickle','wb') as tokenizerFile:\n",
    "    pickle.dump(tokenizer,tokenizerFile)\n",
    "np.save('data.npy',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 256\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "latent_dim = 256\n",
    "\n",
    "autoencoder = build_autoencoder(input_length,vocab_size,latent_dim)\n",
    "autoencoder.compile(optimizer=\"sgd\", loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Embedding (None, None, 256)         65536     \n",
      "_________________________________________________________________\n",
      "encoder (Bidirectional)      (None, 256)               1050624   \n",
      "_________________________________________________________________\n",
      "repeater (RepeatVector)      (None, 256, 256)          0         \n",
      "_________________________________________________________________\n",
      "decoder (Bidirectional)      (None, 256)               1050624   \n",
      "=================================================================\n",
      "Total params: 2,166,784\n",
      "Trainable params: 2,166,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 2 dimensions, but got array with shape (2, 1, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4380f832a146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 validation_data=(test_data, test_data),callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2594\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2596\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    338\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 2 dimensions, but got array with shape (2, 1, 256)"
     ]
    }
   ],
   "source": [
    "filepath=\"sentence_autoencoder.h5py\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "autoencoder.fit(train_data, train_data,\n",
    "                epochs=100,\n",
    "                batch_size=512,\n",
    "                validation_data=(test_data, test_data),callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
