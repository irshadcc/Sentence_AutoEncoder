{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector,Embedding,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextProcesser:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.word_index = dict() ;\n",
    "        self.word_index[\"<PAD>\"] = 0 #Padding\n",
    "        self.word_index[\"<START>\"] = 1 #start\n",
    "        self.word_index[\"<UNK>\"] = 2  # unknown\n",
    "        self.word_index[\"<UNUSED>\"] = 3\n",
    "        self.word_index[\"<EOS>\"] = 4\n",
    "        self.curr_index = 4 ;\n",
    "        \n",
    "        self.rev_word_index = dict()\n",
    "        self.rev_word_index[0] = \"<PAD>\"\n",
    "        self.rev_word_index[1] = \"<START>\"\n",
    "        self.rev_word_index[2] = \"<UNK>\"\n",
    "        self.rev_word_index[3] = \"<UNUSED>\"\n",
    "        self.rev_word_index[4] = \"<EOS>\"\n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_word_list(self,sentence):\n",
    "        \n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        #Remove Punctuation\n",
    "        sentence = sentence.translate(str.maketrans('','','!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'))\n",
    "        \n",
    "        #Remove Whitespaces\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        #Tokenize the sentences \n",
    "        tokens = nltk.tokenize.word_tokenize(sentence) # \n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "    def fit_on_text(self,sentence):\n",
    "        \n",
    "        word_list = self.get_word_list(sentence)\n",
    "        \n",
    "        for word in word_list:\n",
    "            \n",
    "            if word not in self.word_index:\n",
    "                self.curr_index = self.curr_index+1 \n",
    "                \n",
    "                self.word_index[word] = self.curr_index \n",
    "                self.rev_word_index[self.curr_index] = word\n",
    "                \n",
    "                \n",
    "        return word_list\n",
    "        \n",
    "    def get_seq_from_text(self,sentence,max_len=256):\n",
    "        \n",
    "        word_list = self.fit_on_text(sentence)\n",
    "        \n",
    "        seq = [self.word_index[\"<START>\"] ]\n",
    "        for word in word_list:\n",
    "            if word is '.':\n",
    "                seq.append(self.word_index[\"<EOS>\"])\n",
    "            else:\n",
    "                seq.append(self.word_index[word])\n",
    "        \n",
    "        return seq\n",
    "    \n",
    "    def proccess_text(self,file_names=None,text=None,max_len=256):\n",
    "        \n",
    "        seqs = []\n",
    "        \n",
    "        if file_names :\n",
    "            \n",
    "            for file_name in file_names:\n",
    "                with open(filename,'r') as file:\n",
    "                    text = file.read()\n",
    "                    seq = self.get_seq_from_text(text)\n",
    "                seqs.append(seq)\n",
    "        elif text:\n",
    "            \n",
    "            seq = self.get_seq_from_text(text)\n",
    "            seqs.append(seq)\n",
    "        else:\n",
    "            print(\"file_names = None and text = None\")\n",
    "            \n",
    "        seqs = sequence.pad_sequences(seqs,value=self.word_index[\"<PAD>\"],padding='post',maxlen=max_len)\n",
    "        return seqs\n",
    "    \n",
    "    def decode_sequence(self,seq):\n",
    "        \"\"\"\n",
    "            seq : numpy array of shape = (seq_length,)\n",
    "        \"\"\"\n",
    "        line = []\n",
    "        for i in range(seq.shape[0]):\n",
    "            index = seq[i]\n",
    "            if index >= 0 and index <=4:\n",
    "                \n",
    "                #This block can be used to handle the special values <START> , <PAD> , <EOS> \n",
    "                if index == 4:\n",
    "                    line.append('.')\n",
    "            else:\n",
    "                line.append(self.rev_word_index[index])\n",
    "                \n",
    "        \n",
    "        return ' '.join(line)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_length,vocab_size,latent_dim):\n",
    "\n",
    "    inputs = Input(shape=(None,))\n",
    "\n",
    "    x = Embedding(vocab_size,latent_dim,input_length=input_length,name=\"encoder_embedding\")(inputs)\n",
    "\n",
    "    encoded = Bidirectional(LSTM(latent_dim), merge_mode=\"sum\",name=\"encoder\")(x)\n",
    "\n",
    "\n",
    "    decoder_input = RepeatVector(input_length, name=\"repeater\")(encoded)\n",
    "    decoded = Bidirectional(LSTM(latent_dim), merge_mode=\"sum\",name=\"decoder\")(decoder_input)\n",
    "\n",
    "    autoencoder = Model(inputs,decoded)\n",
    "    encoder = Model(inputs,encoded)\n",
    "    \n",
    "    return autoencoder,encoder\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextProcesser()\n",
    "def load_dataset():\n",
    "    \n",
    "    train_data = []\n",
    "    with open('Data/desc.txt','r') as file:\n",
    "        for line in file:\n",
    "            data = tokenizer.proccess_text(text=line)\n",
    "            train_data.extend([data])\n",
    "        \n",
    "    return np.array(train_data)\n",
    "\n",
    "data = load_dataset()\n",
    "data = data.reshape(data.shape[0],data.shape[2])\n",
    "\n",
    "with open('tokenizer.pickle','wb') as tokenizerFile:\n",
    "    pickle.dump(tokenizer,tokenizerFile)\n",
    "np.save('data.npy',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 256\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "latent_dim = 256\n",
    "\n",
    "autoencoder,encoder = build_autoencoder(input_length,vocab_size,latent_dim)\n",
    "autoencoder.compile(optimizer=\"sgd\", loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"sentence_autoencoder.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "autoencoder.fit(train_data, train_data,\n",
    "                epochs=100,\n",
    "                batch_size=512,\n",
    "                validation_data=(test_data, test_data),callbacks=callbacks_list)\n",
    "\n",
    "encoder.save(\"sentence_encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load Encoder Model and generate latent representation\n",
    "# encoder = keras.models.load_model(\"sentence_encoder.h5\")\n",
    "# encoder.predict(<(batch_length,seq_length)>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
